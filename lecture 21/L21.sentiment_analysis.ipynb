{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1564b354",
   "metadata": {},
   "source": [
    "## Dataset Overview: Movie Review Sentiment Analysis\n",
    "\n",
    "The dataset contains movie reviews and their corresponding sentiment labels. The sentiment labels indicate whether the review is positive or negative based on the content of the review.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "* **Text:** The movie review text containing opinions, critiques, and evaluations of the movie.\n",
    "* **Label:** The sentiment label of the review (1 = Positive, 0 = Negative)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85f76d3",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a51e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e0e1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts_neg = 5,331\n",
      "\n",
      " simplistic , silly and tedious . \n",
      "\n",
      " it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      " exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      " [garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      " a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n"
     ]
    }
   ],
   "source": [
    "fn='data/rt-polarity.neg'\n",
    "\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f: # some invalid symbols encountered\n",
    "    texts_neg  = f.read().splitlines()\n",
    "\n",
    "print ('len of texts_neg = {:,}'.format (len(texts_neg )))\n",
    "for review in texts_neg [:5]:\n",
    "    print ( '\\n', review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7122ab95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of texts_pos = 5,331\n",
      "\n",
      " the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      " the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      " effective but too-tepid biopic\n",
      "\n",
      " if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      " emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n"
     ]
    }
   ],
   "source": [
    "fn='data/rt-polarity.pos'\n",
    "\n",
    "with open(fn, \"r\",encoding='utf-8', errors='ignore') as f:\n",
    "    texts_pos   = f.read().splitlines()\n",
    " \n",
    "print ('len of texts_pos = {:,}'.format (len(texts_pos)))\n",
    "for review in texts_pos [:5]:\n",
    "    print ('\\n', review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327dc68b",
   "metadata": {},
   "source": [
    "#### Assign labels (0 for negative, 1 for positive) and combine the datasets into a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d26ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg = pd.DataFrame({'text': texts_neg, 'label': 0})\n",
    "df_pos = pd.DataFrame({'text': texts_pos, 'label': 1})\n",
    "\n",
    "df = pd.concat([df_neg, df_pos], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b61b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73edee27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this film seems thirsty for reflection , itsel...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the movie's thesis -- elegant technology for t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tries too hard to be funny in a way that's too...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>disturbingly superficial in its approach to th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>an ugly , pointless , stupid movie .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>neither a rousing success nor a blinding embar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ice age posits a heretofore unfathomable quest...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the hours , a delicately crafted film , is an ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>the tenderness of the piece is still intact .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>occasionally , in the course of reviewing art-...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  this film seems thirsty for reflection , itsel...      1\n",
       "1  the movie's thesis -- elegant technology for t...      1\n",
       "2  tries too hard to be funny in a way that's too...      0\n",
       "3  disturbingly superficial in its approach to th...      0\n",
       "4              an ugly , pointless , stupid movie .       0\n",
       "5  neither a rousing success nor a blinding embar...      0\n",
       "6  ice age posits a heretofore unfathomable quest...      0\n",
       "7  the hours , a delicately crafted film , is an ...      1\n",
       "8     the tenderness of the piece is still intact .       1\n",
       "9  occasionally , in the course of reviewing art-...      1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "603e6d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    5331\n",
       "0    5331\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c29a09",
   "metadata": {},
   "source": [
    "### Split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a9e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6d48ba",
   "metadata": {},
   "source": [
    "## Text Classification Methods\n",
    "\n",
    "For text classification, will be use the following methods:\n",
    "\n",
    "### Text Vectorization:\n",
    "\n",
    "* **TF-IDF**: Computes the importance of words based on term frequency and document frequency.\n",
    "* **CountVectorizer**: Creates a word count matrix.\n",
    "* **Word2Vec**: Generates vector representations of words based on context.\n",
    "\n",
    "### Classification Model:\n",
    "\n",
    "* **Logistic Regression**: A linear model for binary classification tasks.\n",
    "\n",
    "The performance metrics for each method (precision, recall, F1-score) will be evaluate on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c73a28",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "588f9276",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac48d11d",
   "metadata": {},
   "source": [
    "#### Train model on features extracted by tfidf vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6f2721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.74      0.75      1098\n",
      "           1       0.73      0.77      0.75      1035\n",
      "\n",
      "    accuracy                           0.75      2133\n",
      "   macro avg       0.75      0.75      0.75      2133\n",
      "weighted avg       0.75      0.75      0.75      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=2000)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "print(\"TF-IDF\")\n",
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961f590",
   "metadata": {},
   "source": [
    "### CountVectorizer vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e310e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer =  CountVectorizer(ngram_range=(1, 2), max_df=0.9, min_df=2, max_features=5000)\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "X_test_count = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4e55b5",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc6ce8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.74      0.75      1098\n",
      "           1       0.73      0.75      0.74      1035\n",
      "\n",
      "    accuracy                           0.74      2133\n",
      "   macro avg       0.74      0.74      0.74      2133\n",
      "weighted avg       0.74      0.74      0.74      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf  = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_count, y_train)\n",
    "y_pred_count = clf.predict(X_test_count)\n",
    "\n",
    "print(\"CountVectorizer\")\n",
    "print(classification_report(y_test, y_pred_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4aa6e6",
   "metadata": {},
   "source": [
    "### Word Embeddings with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ac34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text.lower())\n",
    "\n",
    "# Prepare sentences for Word2Vec (it needs a list of tokenized sentences)\n",
    "X_train_tokenized = [tokenize_text(text) for text in X_train]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(X_train_tokenized,\n",
    "                    vector_size=100,     # Dimensionality of word vectors\n",
    "                    window=5,            # Context window size\n",
    "                    min_count=5,         # Ignore words with frequency below this\n",
    "                    workers=4)           # Number of threads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "828f3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3816\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(w2v_model.wv.key_to_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82562c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create document vectors by averaging word vectors\n",
    "def document_vector(doc, model):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.wv]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean([model.wv[word] for word in doc], axis=0)\n",
    "\n",
    "# Create document vectors for train and test sets\n",
    "X_train_w2v = np.array([document_vector(doc, w2v_model) for doc in X_train_tokenized])\n",
    "X_test_tokenized = [tokenize_text(text) for text in X_test]\n",
    "X_test_w2v = np.array([document_vector(doc, w2v_model) for doc in X_test_tokenized])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5ec9d",
   "metadata": {},
   "source": [
    "#### Train a logistic regression model on Word2Vec features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4d9ea5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.54      0.57      1098\n",
      "           1       0.56      0.62      0.59      1035\n",
      "\n",
      "    accuracy                           0.58      2133\n",
      "   macro avg       0.58      0.58      0.58      2133\n",
      "weighted avg       0.58      0.58      0.58      2133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w2v_clf = LogisticRegression(max_iter=1000).fit(X_train_w2v, y_train)\n",
    "w2v_predictions = w2v_clf.predict(X_test_w2v)\n",
    "w2v_scores = w2v_clf.predict_proba(X_test_w2v)[:, 1]\n",
    "\n",
    "print(f\"Word2Vec\")\n",
    "print(classification_report(y_test, w2v_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e01117e",
   "metadata": {},
   "source": [
    "## Model Comparison: Sentiment Analysis\n",
    "\n",
    "### Methods Used:\n",
    "\n",
    "1. **TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "   - **Precision:** 0.77\n",
    "   - **Recall:** 0.74\n",
    "   - **F1-Score:** 0.75\n",
    "   - **Accuracy:** 0.75\n",
    "   - **Macro Average:** 0.75 (Precision, Recall, F1-Score)\n",
    "   - **Weighted Average:** 0.75 (Precision, Recall, F1-Score)\n",
    "\n",
    "2. **CountVectorizer:**\n",
    "   - **Precision:** 0.77\n",
    "   - **Recall:** 0.76\n",
    "   - **F1-Score:** 0.76\n",
    "   - **Accuracy:** 0.76\n",
    "   - **Macro Average:** 0.76 (Precision, Recall, F1-Score)\n",
    "   - **Weighted Average:** 0.76 (Precision, Recall, F1-Score)\n",
    "\n",
    "3. **Word2Vec:**\n",
    "   - **Precision:** 0.60\n",
    "   - **Recall:** 0.54\n",
    "   - **F1-Score:** 0.57\n",
    "   - **Accuracy:** 0.58\n",
    "   - **Macro Average:** 0.58 (Precision, Recall, F1-Score)\n",
    "   - **Weighted Average:** 0.58 (Precision, Recall, F1-Score)\n",
    "\n",
    "### Conclusion:\n",
    "- **TF-IDF and CountVectorizer** showed comparable performance, with **CountVectorizer** slightly outperforming **TF-IDF** in terms of recall and F1-score.\n",
    "- **Word2Vec** performed significantly worse than both TF-IDF and CountVectorizer in all evaluation metrics, with lower precision, recall, and accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
